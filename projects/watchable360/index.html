<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
        <title>Watchable 360°</title>

        <!-- Bootstrap -->
        <link href="http://sammy-su.github.io/css/bootstrap.min.css" rel="stylesheet">
        <link href="http://sammy-su.github.io/css/custom.css" type="text/css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
            <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->
    </head>

    <body>

        <div class="container">
            <nav class="navbar navbar-default navbar-static-top">
                <div class="container-fluid">
                    <div class="navbar-header">
                        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand" href="http://sammy-su.github.io">
                            <span class="glyphicon glyphicon-home" aria-hidden="true"></span>
                        </a>
                    </div>

                    <div id="navbar" class="navbar-collapse collapse">
                        <ul class="nav navbar-nav">
                            <li><a href="#improvement">Improvements</a></li>
                            <li><a href="#video">Videos</a></li>
                            <li><a href="#publication">Publication</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </nav>

            <div class="row">
                <div class="row">
                    <h1>
                        <p class="text-center">Making 360° Video Watchable in 2D:</p>
                        <p class="text-center">Learning Videography for Click Free Viewing</p>
                    </h1>
                </div>
                <div class="row">
                    <img src="figures/concept.png" class="img-responsive center-block" alt="Concept figure" style="max-width:92%; width:640px;">
                </div>
                <p style="font-size:18px; margin-top:18px">
                    In our  <a href="http://sammy-su.github.io/projects/Pano2Vid/">prior work</a>,
                    we propose the Pano2Vid problem that aims to generate normal-field-of-view (NFOV) videos that look like human captured given a 360° video.
                    We propose the AutoCam algorithm that solves the Pano2Vid problem by learning to control virtual camera within 360° video axis from human captured NFOV videos.
                </p>
                <p style="font-size:18px; margin-top:18px">
                    In this work, we propose three improvements over the AutoCam algorithm.
                    First, we generalize the task of Pano2Vid to allow changes in the field-of-view (FOV), i.e. zooming,
                    which is a commonly used techniqe in videography.
                    Second, we present a coarse-to-fine trajectory search algorithm that iteratively refines the camera control while reducing the search space to improve the computational efficiency.
                    Finally, we generate a diverse set of output videos given an input 360° video to account for the fact that valid Pano2Vid solutions are often multimodal.
                </p>
                [<a href="#">top</a>]
            </div>

            <div class="row" id="improvement">
                <h2>Improvements over AutoCam</h2>
                <hr class="single">
                <h3>Zoom Lens</h3>
                <p style="font-size:18px; margin-top:18px">
                    The new algorithm enables zooming in virtual camera control.
                    Zooming not only makes the camera control more natural but also improves the quality of capture-worthiness score.
                </p>
                <div class="row">
                    <img src="figures/glimpses.png" class="img-responsive center-block" alt="Zoom Lens" style="max-width:72%; width:420px;">
                </div>
                <h3>Coarse to Fine Trajectory Search</h3>
                <p style="font-size:18px; margin-top:18px">
                    The AutoCam algorithm finds the camera trajectories over all candidate glimpses.
                    It has to process all glimpses, which is computationally intensive.
                    The new algorithm imporves computational efficiency by using a two stage trajectory search algorithm to avoid processing all glimpses.
                </p>
                <div class="row">
                    <img src="figures/coarse2fine.gif" class="img-responsive center-block" alt="Coarse to Fine Search" style="max-width:92%; width:960px;">
                </div>
                <h3>Diverse Trajectory Search</h3>
                <p style="font-size:18px; margin-top:18px">
                    The original AutoCam algorithm may generate redundant outputs such that the camera trajectories are almost identical.
                    The new algorithm search the trajectories iteratively and encourage the diversity between outputs.
                </p>
                <div class="row">
                    <img src="figures/diverse.gif" class="img-responsive center-block" alt="Diverse Trajectory Search" style="max-width:92%; width:960px;">
                </div>
                [<a href="#">top</a>]
            </div>

            <div class="row" id="video">
                <h2>Video Examples</h2>
                <hr class="single">
                <div class="row">
                    <div class="col-md-6" style="margin-top:8px">
                        <div class="embed-responsive embed-responsive-16by9">
                            <video controls>
                                 <source src="http://vision.cs.utexas.edu/projects/watchable360/videos/JwtM43kA6Do.mp4" type="video/mp4">
                            </video>
                        </div>
                        <p style="font-size:18px; margin-top:18px">
                            Zooming allows the algorithm to emphasize particular object and moment in the video.
                        </p>
                    </div>
                    <div class="col-md-6" style="margin-top:8px">
                        <div class="embed-responsive embed-responsive-16by9">
                            <video controls>
                                 <source src="http://vision.cs.utexas.edu/projects/watchable360/videos/itS1LuDznDQ-fast.mp4" type="video/mp4">
                            </video>
                        </div>
                        <p style="font-size:18px; margin-top:18px">
                            This example shows why it is important to generate diverse trajectories from the same input video.
                            The two outputs demonstrate different ways to capture a video in the same scene.
                        </p>
                    </div>
                </div>
                <div class="row" style="margin-top:21px">
                    <div class="col-md-6" style="margin-top:8px">
                        <div class="embed-responsive embed-responsive-16by9">
                            <video controls>
                                 <source src="http://vision.cs.utexas.edu/projects/watchable360/videos/1PtgREFVyfM-trim.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="col-md-6" style="margin-top:8px">
                        <div class="embed-responsive embed-responsive-16by9">
                            <video controls>
                                 <source src="http://vision.cs.utexas.edu/projects/watchable360/videos/p4BTUFwzJDs-trim.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="col-md-12" style="margin-top:4px">
                        <p style="font-size:18px; margin-top:18px">
                            These two examples show that zooming helps to learn a better capture-worthiness model.
                            The content captured by the new algorithm is more interesting in both cases.
                        </p>
                    </div>
                </div>

                <h3>Failure Cases</h3>
                <div class="row">
                    <div class="col-md-6 col-md-offset-3">
                        <div class="embed-responsive embed-responsive-16by9">
                            <video controls>
                                 <source src="http://vision.cs.utexas.edu/projects/watchable360/videos/Ss8zC_FjVBo-fast.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="col-md-12">
                        <p style="font-size:18px; margin-top:4px">
                            In this example, the algorithm focus on the videographers,
                            but the players on the playground should be more importatnt than the videographers.
                            The algorithm does not reason about the importance of different objects in the scene,
                            and further information is necessary to solve the problem.
                        </p>
                    </div>
                </div>

                <h3>Annotation Interface</h3>
                <div class="row">
                    <div class="col-md-6">
                        <div class="embed-responsive embed-responsive-16by9">
                            <video controls>
                                 <source src="http://vision.cs.utexas.edu/projects/watchable360/videos/nT5ete4d8v0-rec1-traj0.mp4" type="video/mp4">
                            </video>
                        </div>
                        <p style="font-size:18px; margin-top:18px">
                        </p>
                    </div>
                    <div class="col-md-6">
                        <div class="embed-responsive embed-responsive-16by9">
                            <video controls>
                                 <source src="http://vision.cs.utexas.edu/projects/watchable360/videos/nT5ete4d8v0-rec1-traj1.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <div class="col-md-12">
                        <p style="font-size:18px; margin-top:18px">
                            These are two trajectories annotated by the same editor.
                            Note the orientation of the 360° video is shifted by 180° in the second example.
                            This encourage the editor to annotate different trajectories and avoid the bias introduced by the interface.
                        </p>
                    </div>
                </div>
                [<a href="#">top</a>]
            </div>

            <div class="row" id="publication">
                <h2>Publication</h2>
                <hr class="single">
                <ul style="font-size:18px">
                    <li>
                    Yu-Chuan Su, Kristen Grauman,
                    "Making 360° Video Watchable in 2D: Learning Videography for Click Free Viewing,"
                    CVPR 2017 (Spotlight)<br>
                    [<a href="https://arxiv.org/abs/1703.00495">arXiv</a>]
                    [<a href="https://www.dropbox.com/s/sxoxxwwp69ta7eb/watchable360_spotlight.mp4?dl=1">spotlight</a>]
                    [<a href="cvpr2017su-poster.pdf">poster</a>]
                    [<a href="https://github.com/sammy-su/Pano2Vid">code</a>]
                    [<a href="https://www.dropbox.com/s/pn948465a9he30q/human_edit.json?dl=1">HumanEdit</a>]
                    </li>
                </ul>
                [<a href="#">top</a>]
            </div>
        </div> <!-- /container -->

        <footer>
        </footer>

        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="http://sammy-su.github.io/js/bootstrap.min.js"></script>
        <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
         (i[r].q=i[r].q||[]).push(arguments)
         },i[r].l=1*new Date();a=s.createElement(o),
         m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-37193013-6', 'auto');
        ga('send', 'pageview');
        </script>
    </body>
</html>
